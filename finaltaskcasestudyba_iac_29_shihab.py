# -*- coding: utf-8 -*-
"""FinalTaskCaseStudyBA_IAC_29_Shihab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12cmv5ge-d1q4sJZWgcOSrsB5xqh8oZPk

#Case Study: Supply and Sale Management with Business Intelligence

------------------------------------------------------------------------------

Name: Mohammad Shihab Uddin

ID: 164138

Course: Business Analytics

Batch: 29

Contact: 01673011922

Email: shihabshahidiiuc@gmail.com

This analysis aims to answer the following questions based on

A. Descriptive analysis,

B. Predictive analysis, and

C. Prescriptive analysis.

1.   Select the facts, the dimensions, the fact table, the dimensional tables for the above analysis.
2.   Perform the above three analyses on specified items (I00151, I00085).
3.   Perform the above three analyses on a store (S0001).
4.   Recommend the feasibility of a new store in a certain area.
5.   Classify the customers based on the purchase and personal profile.
6.   Recommend the advertisement of an item to the customers based on the classification of the customers.
7. Recommend anything else that you would like to increase profit or decrease losses. There are no restrictions in this case, the more you explore, the better.

#Importing the libraries

------------------------------------------------------------------------------
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, confusion_matrix, r2_score, accuracy_score, classification_report

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA
from pandas.tseries.offsets import DateOffset
from statsmodels.tsa.statespace.sarimax import SARIMAX

"""#Adding the dataset

------------------------------------------------------------------------------
"""

excel_file = pd.ExcelFile('case-study-data.xlsx')

"""#Exploring the File

------------------------------------------------------------------------------
"""

sheet_names = excel_file.sheet_names
print('Number of sheet: ',len(sheet_names))
print('Sheet names: ',sheet_names)

for sheet in sheet_names:
  print('------',sheet,'------')
  sheet = pd.read_excel('case-study-data.xlsx',sheet_name=sheet,engine='openpyxl')
  print('Number of (rows, columns): ',sheet.shape)
  print('Column Names: ',sheet.columns.tolist())

"""The file contains the following sheets, which align with the requirements of the case study:

1. Fact_table - Likely contains the main transactional data or transaction details including customer, time, item, store, quantity sold, unit, unit price, and total price.

    It has 100000 rows and 9 columns, relation with other 5 sheets by payment_key, item_key,customer_key, time_key and store_key columns respectively
2. Trans_dim - Probably includes details about transactions, Provides information about the payment method, including transaction type and associated bank name.

   It has 39 rows and 3 columns, relation with Fact_table sheet by payment_key column
3. Item_dim - Contains item-related information. Details about items, including item name, type, price, manufacturing country, supplier, stock quantity, and unit.

   It has 264 rows and 8 columns, relation with Fact_table sheet by item_key column
4. Customer_dim - Contains customer-related information. Contains customer information like name, contact details, address, and district

   It has 9191 rows and 9 columns, relation with Fact_table sheet by customer_key column
5. Time_dim - Includes time-related data for each transaction. Stores time-related information such as date, hour, day, week, month, quarter, and year.

   It has 4999 rows and 8 columns, relation with Fact_table sheet by time_key column
6. Store_dim - Contains store-related information. Provides information about stores including size, location, city, upazila, district, and division.

   It has 44 rows and 7 columns, relation with Fact_table sheet by store_key column

#Merging the sheets

------------------------------------------------------------------------------
"""

merge_sheet = ""
i=1
merge_sheet = pd.read_excel('case-study-data.xlsx',sheet_name=sheet_names[0],engine='openpyxl')
columns =merge_sheet.columns.tolist()
while i<len(sheet_names):
    sheet_1 = pd.read_excel('case-study-data.xlsx',sheet_name=sheet_names[i],engine='openpyxl')
    columns_1 =sheet_1.columns.tolist()
    columns_2 =merge_sheet.columns.tolist()
    j=0
    match_col = ""
    while j<len(columns_1)-1:
      if columns_1[0] == columns_2[j]:
        match_col = columns_2[j]
        break
      j+=1
    merge_sheet = pd.merge(merge_sheet,sheet_1,on=match_col)
    i+=1

print('Number of (rows, columns): ',merge_sheet.shape)
print('Column Names: ',merge_sheet.columns)

"""After the doing merge, there are Number of rows 100000 and columns 39, By default some column name have been renamed addition with suffix _x or _y due to exactly match column name between sheets

#Rename Columns

------------------------------------------------------------------------------
"""

merge_sheet.rename(columns={'unit_x': 'unit_fact', 'unit_price_x': 'unit_price_fact'}, inplace=True)
merge_sheet.rename(columns={'unit_y': 'unit_item', 'unit_price_y': 'unit_price_item'}, inplace=True)
merge_sheet.rename(columns={'upazila_x': 'upazila_customer', 'district_x': 'district_customer', 'division_x': 'division_customer'}, inplace=True)
merge_sheet.rename(columns={'upazila_y': 'upazila_store', 'district_y': 'district_store', 'division_y': 'division_store'}, inplace=True)

print('Number of (rows, columns): ',merge_sheet.shape)
print('Column Names: ',merge_sheet.columns)

"""Removing ambiguity or easy finding column name has been changed In merge sheet with related sheet name instead of addition with suffix _x or _y column name

#Understanding the data

------------------------------------------------------------------------------
"""

merge_sheet.info()

"""There are 28 object type columns or string data and 11 number type columns or numeric data,

In number type data column, sold quantity, unit price, total price and stock quantity are effective summarize,

other 6 number type data column are related to date breakdown

#Dropping irrelevant columns

------------------------------------------------------------------------------
"""

columns_to_drop = ['payment_key', 'customer_key', 'time_key','bank_name','unit_fact','unit_item','contact_no','nid','street']
merge_sheet = merge_sheet.drop(columns=columns_to_drop)

print('Number of (rows, columns): ',merge_sheet.shape)
print('Column Names: ',merge_sheet.columns)

"""#Handling null values

------------------------------------------------------------------------------
"""

merge_sheet.isnull().sum()

#merge_sheet.dropna(inplace=True)

print('Number of (rows, columns): ',merge_sheet.shape)

"""The data has been cleaned, and now we have 30 columns, focusing on store, sales, product details, and time-related features. There are minimal missing values, mostly in the name column, which we can ignore for now as it doesn't significantly impact the analysis.

#Observing the correlation

------------------------------------------------------------------------------
"""

merge_sheet.corr(method='pearson', min_periods=1, numeric_only=True)

sns.heatmap(merge_sheet.corr(method='pearson', min_periods=1, numeric_only=True), cmap = 'viridis')

"""There are good correlation between sold quantity, unit price and total price only

#Statistics Summary

------------------------------------------------------------------------------
"""

merge_sheet[['quantity_sold','unit_price_fact','total_price']].describe()

"""**Quantity**: The average Quantity figure was 5.99. The minimum recorded quantity was 1, and the maximum was 11, with the middle value (median) being 6. The quantity data shows a standard deviation of 3.1.

**Unit Price**: The average Unit Price was 16.95. The minimum recorded Unit Price was 6, and the maximum was 55, with the middle value (median) being 15. The Unit Price data shows a standard deviation of 7.49.

**Total Price**: The average total Price was 101.65. The minimum recorded Total Price was 6, and the maximum was 605, with the middle value (median) being 90. The Total Price data shows a standard deviation of 73.81.

There are no null value

#Visualizing the total Sales by Store

------------------------------------------------------------------------------
"""

total_sales_by_store = merge_sheet.groupby('store_key')['total_price'].sum().sort_values(ascending=False)
plt.figure(figsize=(12, 5))
plt.bar(total_sales_by_store.index, total_sales_by_store.values,color='#1E2A5E')
plt.xlabel('Store Key')
plt.ylabel('Total Sales')
plt.title('Total Sales by Store')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],rotation=90,label_type='center',color='white')
plt.show()

"""From above graph, finding the location of highest revenue store"""

S0039_store_location = merge_sheet.query("store_key=='S0039'")["division_store"].drop_duplicates()
S0039_store_location

"""From above graph, finding the location of my assign store"""

S0001_store_location = merge_sheet.query("store_key=='S0001'")["division_store"].drop_duplicates()
S0001_store_location

"""From above chart shows the total sales by store, where my assign store (S0001) is 19th position out of 44, located in Dhaka and top most one in Chittagong

#Visualizing the total Sales by Area (Store division)

------------------------------------------------------------------------------
"""

total_sales_by_store_division = merge_sheet.groupby('division_store')['total_price'].sum().sort_values(ascending=True)
plt.figure(figsize=(5, 5))
plt.barh(total_sales_by_store_division.index, total_sales_by_store_division.values,color='#A02334')
plt.xlabel('Total Sales')
plt.ylabel('Store division')
plt.title('Total Sales by Area (store division)')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],label_type='center',color='white')
plt.show()

"""Dhaka is the most significant sales area and, other location moderately close,ranging from about 0.9 million to 0.7 million, mymensingh is the lowest sales division

#Visualizing the total number of Store by Location (Store division)

------------------------------------------------------------------------------
"""

total_count_by_store_division = merge_sheet.groupby('division_store')['store_key'].nunique().sort_values(ascending=False)
plt.figure(figsize=(5, 5))
plt.bar(total_count_by_store_division.index, total_count_by_store_division.values,color='#1A4870')
plt.xlabel('Number of Store')
plt.ylabel('Store division')
plt.title('#Visualizing the total number of Store by Location (Store division)')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],label_type='center',color='white')
plt.show()

"""Most of the store (17) in dhaka division and other between 5 to 3 out of 44

#Visualizing the total Sales by Item

------------------------------------------------------------------------------
"""

total_sales_by_item = merge_sheet.groupby('item_name')['total_price'].sum().nlargest(60).sort_values(ascending=False)
plt.figure(figsize=(15, 5))
plt.bar(total_sales_by_item.index, total_sales_by_item.values,color='#674188')
plt.xlabel('Item Name')
plt.ylabel('Total Sales')
plt.title('Total Sales by Item (top 60 out of 264)')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],rotation=90,label_type='center',color='white')
plt.show()

"""From above graph, finding the highest item type"""

item_key_type = merge_sheet.query("item_name=='Tylenol Extra Strength 2 pill packets'")[["item_key","item_type"]].drop_duplicates()
item_key_type

"""From above graph, finding my assign item type"""

I00151_item = merge_sheet.query("item_key=='I00151'")[["item_name","item_type"]].drop_duplicates()
I00151_item

I00085_item = merge_sheet.query("item_key=='I00085'")[["item_name","item_type"]].drop_duplicates()
I00085_item

"""Finding sales value my assign item"""

I00151_item_sales = merge_sheet.query("item_key=='I00151'")['total_price'].sum()
I00151_item_sales

I00085_item_sales = merge_sheet.query("item_key=='I00085'")['total_price'].sum()
I00085_item_sales

"""The most sales Item name is "Tylenol Extra Strength 2 pill packets",

Actualy this is a Medicine, it's vary necessary item

where my assign item name are "Cheetos Flamin' Hot 1 oz" and "Tropicana Orange Juice 10 oz 100%",

these are food and beverage item

#Visualizing the total Sales by Item type

------------------------------------------------------------------------------
"""

total_sales_by_item_type = merge_sheet.groupby('item_type')['total_price'].sum().sort_values(ascending=False)
plt.figure(figsize=(15, 5))
plt.bar(total_sales_by_item_type.index, total_sales_by_item_type.values,color='#E85C0D')
plt.xlabel('Item type')
plt.ylabel('Total Sales')
plt.title('Total Sales by Item Type')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],rotation=90,label_type='center',color='white')
plt.show()

"""#Visualizing the total Quantity Sold by Item

------------------------------------------------------------------------------
"""

total_quantity_sold_by_item = merge_sheet.groupby('item_name')['quantity_sold'].sum().nlargest(60).sort_values(ascending=False)
plt.figure(figsize=(15, 6))
plt.bar(total_quantity_sold_by_item.index, total_quantity_sold_by_item.values,color='#399918')
plt.xlabel('Item Name')
plt.ylabel('Total Quantity Sold')
plt.title('Total Quantity Sold by Item (top 6 out of 264)')
plt.xticks(rotation=90)
plt.bar_label(plt.gca().containers[0],rotation=90,label_type='center',color='white')
plt.show()

"""Finding quantity sold my assign item"""

I00151_item_sales = merge_sheet.query("item_key=='I00151'")['quantity_sold'].sum()
I00151_item_sales

I00085_item_sales = merge_sheet.query("item_key=='I00085'")['quantity_sold'].sum()
I00085_item_sales

"""My assign item quantity "Cheetos Flamin' Hot 1 oz" was 1102 and "Tropicana Orange Juice 10 oz 100%" was 3208

We already saw The most sales Item "Tylenol Extra Strength 2 pill packets", Quantity also most was 18163

#Visualizing the Number of Transaction by Payment Type

------------------------------------------------------------------------------
"""

total_number_of_tran_by_trans_type = merge_sheet.groupby('trans_type').size().sort_values(ascending=False)
fig, ax = plt.subplots()
ax.pie(total_number_of_tran_by_trans_type.values, labels=total_number_of_tran_by_trans_type.index,autopct='%1.1f%%',startangle=90)
ax.axis('equal')
plt.title('Number of Transaction by Payment Type')
plt.show()

"""Most of the transaction was by using card, only 7.3% using cash, we want to increase other transaction type, we can some offer using those type transaction

#Visualizing the Number of Customer by Locaton (Customer division)

------------------------------------------------------------------------------
"""

total_number_of_costomer_by_location = merge_sheet.groupby('division_customer').size()
fig, ax = plt.subplots()
ax.pie(total_number_of_costomer_by_location.values, labels=total_number_of_costomer_by_location.index,autopct='%1.1f%%',startangle=0)
ax.axis('equal')
plt.title('Number of Customer by Location')
plt.show()

"""The most of the customer saty in dhaka, there are no customer in khulna, Rajshahi, Rangpur and Mymensingh, so we can atracting new customer those area giving some offer discount

#Visualizing the total Sales by Year

------------------------------------------------------------------------------
"""

total_sales_by_year = merge_sheet.groupby('year')['total_price'].sum()
plt.plot(total_sales_by_year.index, total_sales_by_year.values)
plt.xlabel('Year')
plt.ylabel('Total Sales')
plt.title('Total Sales by Year')
plt.grid(True)
plt.show()

"""Sales were relatively consistent from 2014 to 2020, ranging from about 1.4 million to 1.5 million.
The year 2018 saw the highest total sales, with approximately 1.5 million.
Thereâ€™s a significant drop in 2021, likely due to incomplete data

#Visualizing the total Sales by Quarter

------------------------------------------------------------------------------
"""

total_sales_by_quarter = merge_sheet.groupby('quarter')['total_price'].sum()
plt.plot(total_sales_by_quarter.index, total_sales_by_quarter.values,color='red')
plt.xlabel('Quarter')
plt.ylabel('Total Sales')
plt.title('Total Sales by Quarter')
plt.grid(True)
plt.show()

"""Sales show dramatic fluctuations across quarters, with no single quarter consistently outperforming others across years.
Q4 generally tends to have higher sales

#Visualizing the total Sales by Month

------------------------------------------------------------------------------
"""

total_sales_by_month = merge_sheet.groupby('month')['total_price'].sum()
plt.plot(total_sales_by_month.index, total_sales_by_month.values,color='#0D7C66')
plt.xlabel('Month')
plt.ylabel('Total Sales')
plt.title('Total Sales by Month')
plt.grid(True)
plt.show()

"""Sales vary month to month with no consistent pattern across years, september is lowest and october is highest

#Visualizing the total Sales by Week day

------------------------------------------------------------------------------
"""

merge_sheet['date'] = pd.to_datetime(merge_sheet['date'], format='%d-%m-%Y %H:%M')
total_sales_by_day = merge_sheet.groupby(merge_sheet['date'].dt.day_name())['total_price'].sum()
plt.plot(total_sales_by_day.index, total_sales_by_day.values,color='#2E073F')
plt.xlabel('Day of the Week')
plt.ylabel('Total Sales')
plt.title('Total Sales by Week day')
plt.grid(True)
plt.show()

"""Friday having the highest sales and Tuesday the lowest over the week

#Visualizing the Total Sales by Date of month

------------------------------------------------------------------------------
"""

total_sales_by_day = merge_sheet.groupby(['day'])['total_price'].sum()
plt.plot(total_sales_by_day.index, total_sales_by_day.values,color='#914F1E')
plt.xlabel('date of the month')
plt.ylabel('Total Sales')
plt.title('Total Sales by date of the month')
plt.grid(True)
plt.show()

"""In date of 8 is highest and end ofthe month is lowest

#Visualizing the Total Sales by Location (Customer vs Store, division)

------------------------------------------------------------------------------
"""

total_trans_count_by_store_division = merge_sheet.groupby(['division_store'])['total_price'].count()
total_trans_count_by_customer_division = merge_sheet.groupby(['division_customer'])['total_price'].count()
total_trans_count_by_division = pd.concat([total_trans_count_by_store_division, total_trans_count_by_customer_division], axis=1)
total_trans_count_by_division.columns = ['Store', 'Customer']
plt.bar(total_trans_count_by_division.index, total_trans_count_by_division['Store'], label='Store')
plt.bar(total_trans_count_by_division.index, total_trans_count_by_division['Customer'], label='Customer', bottom=total_trans_count_by_division['Store'])
plt.xlabel('Location')
plt.ylabel('Transaction Count')
plt.title('Transaction Count by Customer Location VS Store Location')
plt.legend()
plt.xticks(rotation=90)
plt.show()

"""Most of the customer location in Dhaka but Compared to that, the number of transaction was more less, so, need to new store open in dhaka, No customer was located in Khulna, Mymensingh, Rajshahi and Rangpur but transaction was happend,in these area, need to discount to increase sales, Barishal and Sylhet were good transaction compare to customer, Chittagong was less transaction compare to customer

#Visualizing the Total Sales by Store Size

------------------------------------------------------------------------------
"""

total_sales_by_store_size = merge_sheet.groupby(['store_size'])['total_price'].sum().sort_values(ascending=False)
plt.figure(figsize=(4, 4))
plt.bar(total_sales_by_store_size.index, total_sales_by_store_size.values,color='#6482AD')
plt.xlabel('Store Size')
plt.ylabel('Total Sales')
plt.title('Total Sales by Store Size)')
plt.bar_label(plt.gca().containers[0],label_type='center',color='white',rotation=90)
plt.show()

"""Sales dosn't matter small and medium size store, when we will open new store, it's size may be either large or small

#Regression Analysis (Single)

------------------------------------------------------------------------------

What is the unit price for what quantity?
"""

df_regression = merge_sheet[['quantity_sold','unit_price_fact']]
df_regression.shape

sns.scatterplot(x='quantity_sold', y='unit_price_fact', data=df_regression)

"""In above chart, Linear regression is not suitable for unit price and quantity

Try to another

What is the total price for what quantity?
"""

df_regression = merge_sheet[['quantity_sold','total_price']]
df_regression.shape

sns.scatterplot(x='quantity_sold', y='total_price', data=df_regression)

"""In above chart, Linear regression is not good but can be done

Separate independent variable (X) and dependent variable (y)
"""

X = df_regression.drop(['quantity_sold'], axis=1)
y = df_regression['quantity_sold']

"""Splite train and test data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)

X.shape

X_train.shape

"""Fit the linear Regrassion model"""

LRModel = LinearRegression()
LRModel.fit(X_train, y_train)

"""Find the coefficient and intercept"""

LRModel.coef_

LRModel.intercept_

"""Prediction"""

ypred = LRModel.predict(X_test)

plt.plot(X_test, ypred, color='r');
plt.scatter(X_test, y_test);

"""Check the score"""

def eval(model, X_train, y_train, X_test, y_test):
    print("The training score is,", model.score(X_train, y_train), end='\n')
    print("The testing score is,", model.score(X_test, y_test))

def metric_score(y_test, ypred):
    print("The mean absolute error is: ", mean_absolute_error(y_test, ypred))
    print("The mean squared error is: ", mean_squared_error(y_test, ypred))
    print("The R2 score is: ", r2_score(y_test, ypred))

eval(LRModel, X_train, y_train, X_test, y_test)

metric_score(y_test, ypred)

"""The score More than this does not seem possible

#Regression Analysis (Multiple)

------------------------------------------------------------------------------

What is the total price for what quantity and unit price?
"""

df_regression_multi = merge_sheet[['unit_price_fact','quantity_sold','total_price']]
df_regression_multi.shape

sns.scatterplot(x='unit_price_fact', y='total_price', data=df_regression_multi)

sns.scatterplot(x='quantity_sold', y='total_price', data=df_regression_multi)

"""In above charts, Linear regression is not good but can be done

Separate independent variable (X) and dependent variable (y)
"""

X = df_regression_multi.drop(['total_price'], axis=1)
y = df_regression_multi['total_price']

"""Splite train and test data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)

"""Fit the linear Regrassion model"""

LRModel.fit(X_train, y_train)

"""Find the coefficient and intercept"""

LRModel.coef_

LRModel.intercept_

"""Prediction"""

ypred = LRModel.predict(X_test)
ypred

"""Check the score"""

eval(LRModel, X_train, y_train, X_test, y_test)

metric_score(y_test, LRModel.predict(X_test))

"""Try to better score using regularization

#Lasso (L1) Regularization

------------------------------------------------------------------------------
"""

LassoModel = Lasso()
LassoModel.fit(X_train, y_train);
LassoModel.score(X_test, y_test)

eval(LassoModel, X_train, y_train, X_test, y_test)

metric_score(y_test, LassoModel.predict(X_test))

"""Score not increased

#Ridge (L2) Regularization

------------------------------------------------------------------------------
"""

RidgeModel = Ridge(alpha=10)
RidgeModel.fit(X_train, y_train)
RidgeModel.score(X_test, y_test)

eval(RidgeModel, X_train, y_train, X_test, y_test)

metric_score(y_test, RidgeModel.predict(X_test))

"""Still score is same

#ElasticNet (hybrid) Regularization

------------------------------------------------------------------------------
"""

ENModel = ElasticNet()
ENModel.fit(X_train, y_train)
ENModel.score(X_test, y_test)

eval(ENModel, X_train, y_train, X_test, y_test)

metric_score(y_test, ENModel.predict(X_test))

"""The score More than this does not possible

#Classification analysis

------------------------------------------------------------------------------
"""

threshold = np.percentile(merge_sheet['total_price'], 75)
merge_sheet['new_store_open_status'] = (merge_sheet['total_price'] > threshold).astype(int)

features = ['division_store', 'store_size', 'trans_type', 'item_type', 'unit_price_fact', 'quantity_sold']
X = merge_sheet[features]
y = merge_sheet['new_store_open_status']

label_encoders = {}
for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])
    label_encoders[column] = le


scaler = StandardScaler()
X[['unit_price_fact', 'quantity_sold']] = scaler.fit_transform(X[['unit_price_fact', 'quantity_sold']])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

log_reg = LogisticRegression(random_state=42)
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42)
svm = SVC(random_state=42)

log_reg.fit(X_train, y_train)
decision_tree.fit(X_train, y_train)
random_forest.fit(X_train, y_train)
svm.fit(X_train, y_train)

y_pred_log_reg = log_reg.predict(X_test)
y_pred_decision_tree = decision_tree.predict(X_test)
y_pred_random_forest = random_forest.predict(X_test)
y_pred_svm = svm.predict(X_test)

def evaluate_model(y_test, y_pred):
    return {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1 Score': f1_score(y_test, y_pred)
    }

results = {
    'Logistic Regression': evaluate_model(y_test, y_pred_log_reg),
    'Decision Tree': evaluate_model(y_test, y_pred_decision_tree),
    'Random Forest': evaluate_model(y_test, y_pred_random_forest),
    'SVM': evaluate_model(y_test, y_pred_svm)
}

results

"""Random Forest and Decision Tre performed the best with an accuracy of 100%,

Logistic Regression and SVM also performed reasonably well but with slightly lower scores.

Based on these results, Random Forest seems to be the most effective model for predicting the success of a new store based on the available features.

#Time series analysis

------------------------------------------------------------------------------

Separate time related data and sales data
"""

df_time_analysis = merge_sheet[['date','year','month','total_price']]
df_time_analysis

"""Prepare date data for month wise aggregation"""

df_time_analysis['date'] = pd.to_datetime(dict(year=df_time_analysis.year, month=df_time_analysis.month, day=1))
df_time_analysis

df_time_analysis = df_time_analysis[['date','total_price']]
df_time_analysis

"""Check data type"""

df_time_analysis.dtypes

"""In time series analysis, need to data sorted first"""

df_time_analysis.sort_values(by=['date'], inplace=True)
df_time_analysis

"""Month wise grouping over the years"""

df_time_analysis= df_time_analysis.groupby('date')['total_price'].sum()
df_time_analysis

"""Convert series to datafram"""

df_time_analysis = pd.DataFrame({'date':df_time_analysis.index, 'total_price':df_time_analysis.values})
df_time_analysis

"""Set index column"""

df_time_analysis.set_index(['date'], inplace=True)
df_time_analysis.head()

"""#Visualizing sales over the time

------------------------------------------------------------------------------
"""

plt.figure(figsize=(20,10))
plt.xlabel("Date")
plt.ylabel("Sales Amount")
plt.plot(df_time_analysis)

test_result = adfuller(df_time_analysis['total_price'])
test_result

"""The line plot above shows the total sales trend over time, aggregated by month and year. It has noise patterns that means random variation in the series.

#Statistical testing (ADF and KPSS) or find the value of d

------------------------------------------------------------------------------
"""

def adfuller_test(data):
    result = adfuller(data)
    labels = ['ADF Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used']
    for value, label in zip(result, labels):
        print(label+' : '+str(value) )
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary")
    else:
        print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

adfuller_test(df_time_analysis['total_price'])

def kpss_test(timeseries):
    print("Results of KPSS Test:")
    kpsstest = kpss(timeseries, regression="c")
    kpss_output = pd.Series(
        kpsstest[0:3], index=["Test Statistic", "p-value", "Lags Used"]
    )
    for key, value in kpsstest[3].items():
        kpss_output["Critical Value (%s)" % key] = value
    print(kpss_output)

    if (kpss_output['p-value'] < 0.05):
        print("The time series is not stationary")
    else:
        print("The time series is stationary")

kpss_test(df_time_analysis['total_price'])

"""Since data is stationary, so d is 0

#Find the value for p and q

------------------------------------------------------------------------------
"""

fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
fig = plot_pacf(df_time_analysis['total_price'].dropna(), lags=40, ax=ax1)
ax2 = fig.add_subplot(212)
fig = plot_acf(df_time_analysis['total_price'].dropna(), lags=40, ax=ax2)

"""From above chart p and q both are 1

#Time Series Models:ARIMA

------------------------------------------------------------------------------
"""

model = ARIMA(df_time_analysis['total_price'], order=(1, 1, 0))
model_fit = model.fit()

model_fit.summary()

df_time_analysis.shape

"""#Forecast the sales over the actual

------------------------------------------------------------------------------
"""

df_time_analysis['forecast'] = model_fit.predict(start=70, end=85)

df_time_analysis[['total_price','forecast']].plot(figsize=(12,8))

"""#Generating future date

------------------------------------------------------------------------------
"""

future_dates=[df_time_analysis.index[-1]+ DateOffset(months=x) for x in range(0, 13)]
future_dates

future_data_df=pd.DataFrame(index=future_dates[1:], columns=df_time_analysis.columns)

future_data_df.head()

future_df = pd.concat([df_time_analysis, future_data_df])

future_df.tail(18)

"""#Forecast the future sales

------------------------------------------------------------------------------
"""

future_df['forecast'] = model_fit.forecast(steps=18)
future_df[['total_price','forecast']].plot(figsize=(12,8))

"""This prediction may be better using SARIMA

#Time Series Models: SARIMA

------------------------------------------------------------------------------
"""

monthly_sales = merge_sheet.groupby(pd.Grouper(key='date', freq='M'))['total_price'].sum()


train_data = monthly_sales[:-12]
test_data = monthly_sales[-12:]

sarima_model = SARIMAX(train_data,
                       order=(1, 1, 0),
                       seasonal_order=(1, 1, 0, 12),
                       enforce_stationarity=False,
                       enforce_invertibility=False)

sarima_fit = sarima_model.fit(disp=False)


forecast = sarima_fit.forecast(steps=12)

plt.figure(figsize=(10, 6))
plt.plot(train_data.index, train_data, label='Training Data')
plt.plot(test_data.index, test_data, label='Actual Sales')
plt.plot(test_data.index, forecast, label='Forecasted Sales', color='red')
plt.title('SARIMA Forecast vs Actual Sales')
plt.legend()
plt.show()

"""The plot is showing how well the model forecasts compared to actual sales data over the time.

#Evaluate the Model

------------------------------------------------------------------------------
"""

rmse = np.sqrt(mean_squared_error(test_data, forecast))
print(f'Root Mean Squared Error: {rmse:.2f}')

print(sarima_fit.summary())

"""#Recommendations

  ------------------------------------------------------------------------------
1. Most of the store (17) in dhaka and other area between 5 to 3 out of 44. we can open new store out side Dhaka
2. Most of the transaction (83.2%) was by using card, 7.3% using cash and 9.5% mobile, we want to increase other transaction type, we can give some offers or  discounts using those type transaction
3. Most of the customer (80.9%) in dhaka, there are no customer in khulna, Rajshahi, Rangpur and Mymensingh, so we can atracting new customer those area giving some offer discount
4. The sales at the beginning of the year are quite low compared to the end of the year so we can give some offers or discounts at the beginning 1st and 2nd quarter
5. Saturday and Tuesday are less sales, we can give some offers or discounts those days
6. To reduce the cost of opening a new store, we can open the small size store because small or medium size store's sales are same
7. Random Forest seems to be the most effective model for predicting the success of a new store based on the available features.
9. SARIMA is better in time series analysis according to pattern
"""